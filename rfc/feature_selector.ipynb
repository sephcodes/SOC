{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_selection, model_selection\n",
    "import joblib\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'soc_202311261432.csv')\n",
    "df = df.fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in\n",
    "                    lst_stopwords]\n",
    "\n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "\n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['JOB_DUTIES'].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df['text_clean'], df['SOC_CODE'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = vectorizer.get_feature_names_out()\n",
    "p_value_limit = 0.99\n",
    "df_features = pd.DataFrame()\n",
    "for cat in np.unique(y_train):\n",
    "    chi2, p = feature_selection.chi2(X_train_vectorized, y_train==cat)\n",
    "    df_features = pd.concat([df_features, pd.DataFrame({'feature':X_names, 'score':1-p, 'y':cat})])\n",
    "    df_features = df_features.sort_values(['y','score'], ascending=[True,False])\n",
    "    df_features = df_features[(df_features['score']>p_value_limit) & (df_features['feature'].str.len() > 10)]\n",
    "    \n",
    "\n",
    "X_names = df_features['feature'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 37-3011.00:\n",
      "Landscaping and Groundskeeping Workers:\n",
      "  . selected features: 2461\n",
      "  . top features: 373011 onetonlineorg,able perform,according instruction,aerating weeding,aerator trimmer,apply mulch,area employment,area jobsites,asked demonstrate,assist sprinkler\n",
      " \n",
      "# 37-3012.00:\n",
      "Pesticide Handlers, Sprayers, and Applicators, Vegetation:\n",
      "  . selected features: 46\n",
      "  . top features: according formula,application,area assist,assist cleaning,backpack sprayer,designated area,herbicide application,job training,spray spread,sprayer spreader\n",
      " \n",
      "# 37-3013.00:\n",
      "Tree Trimmers and Pruners:\n",
      "  . selected features: 75\n",
      "  . top features: appearance health,branch tree,climbing rigging,clipper power,dead excess,excess branch,gain access,ground person,ground tree,hand pruner\n",
      " \n",
      "# 37-3019.00:\n",
      "Grounds Maintenance Workers, All Other:\n",
      "  . selected features: 33\n",
      "  . top features: andor power,care maintenance,continuously,debris ensure,ensure work,general landscaping,landscaping tree,maintenance service,perform wide,safety public\n",
      " \n",
      "# 39-1011.00:\n",
      "Gaming Supervisors:\n",
      "  . selected features: 4\n",
      "  . top features: assist training,employee assist,professional manner,professional\n",
      " \n",
      "# 39-1013.00:\n",
      "First-Line Supervisors of Gambling Services Workers:\n",
      "  . selected features: 22\n",
      "  . top features: noise level,assigned area,depth perception,policy procedure,company policy,providing guest,condition work,skill ability,accountable,duty requested\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for cat in np.unique(y_train)[179:185]:\n",
    "   print('# {}:'.format(cat))\n",
    "   print('{}:'.format(df[df['SOC_CODE'] == cat].iloc[0]['SOC_TITLE']))\n",
    "   print('  . selected features:',\n",
    "         len(df_features[df_features['y']==cat]))\n",
    "   print('  . top features:', ','.join(\n",
    "df_features[df_features['y']==cat]['feature'].values[:10]))\n",
    "   print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = joblib.load('random_forest_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Get a sample instance for explanation\n",
    "# text = \"Reline or repair interior of refractory vessels with refractory clay and other refractory material. Chip slag from linings of ladles or remove linings when beyond repair, using hammers and chisels and other equipment. Mix specified amounts of sand, clay, mortar powder, and water to form refractory clay or mortar, using shovels or mixing machines. Tighten locknuts holding refractory stopper assemblies together, spread mortar on jackets to seal sleeve joints, and dry mortar. Remove worn or damaged block refractory linings of furnaces, kilns, and cyclones using hand tools and other equipment. Climb scaffolding, carrying hoses, and spray surfaces with refractory mixtures, using spray equipment. Construct and installation of refractory material laying by hand and spraying equipment. May perform other duties and tasks per SOC Code 49-9045.\"\n",
    "# text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "# # Initialize the SHAP explainer\n",
    "# explainer = shap.Explainer(classifier)\n",
    "\n",
    "# # Get SHAP values for the sample instance\n",
    "# shap_values = explainer.shap_values(text_vectorized)\n",
    "\n",
    "# # Get feature names\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Identify important features based on SHAP values\n",
    "# important_features = [feature_names[i] for i in range(len(shap_values[0])) if abs(shap_values[0][i]) > 0.95]\n",
    "\n",
    "# # Print the predicted class and important features\n",
    "# predicted_class = classifier.predict(text_vectorized)[0]\n",
    "# print(f\"Predicted Class: {predicted_class}\")\n",
    "# print(\"Important Features:\", important_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
